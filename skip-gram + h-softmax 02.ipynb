{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from h_softmax import Tree\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus example: [['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']]\n",
      "vocab size: 8\n",
      "preprocessed: [[0, 1, 2, 3, 4, 5, 0, 6, 7]]\n"
     ]
    }
   ],
   "source": [
    "corpus = [\"the quick brown fox jumps over the lazy dog\".split(' ')]\n",
    "print(f'corpus example: {corpus}')\n",
    "\n",
    "words = Counter()\n",
    "for doc in corpus:\n",
    "    for word in doc:\n",
    "        words[word] += 1\n",
    "\n",
    "vocab_size = len(words)\n",
    "print(f'vocab size: {vocab_size}')\n",
    "\n",
    "word_to_ix, ix_to_word = {}, {}\n",
    "for ix, (word, _) in enumerate(words.most_common()):\n",
    "    word_to_ix[word] = ix\n",
    "    ix_to_word[ix] = word\n",
    "\n",
    "corpus_preprocessed = list()\n",
    "for document in corpus:\n",
    "    corpus_preprocessed.append([word_to_ix[word] for word in document])\n",
    "print(f'preprocessed: {corpus_preprocessed}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(corpus, window_size=2):\n",
    "    for document in corpus:\n",
    "        doc_len = len(document)\n",
    "        for ix in range(doc_len):\n",
    "            floor = max(0, ix-window_size)\n",
    "            ceil = min(doc_len, ix+window_size+1)\n",
    "            \n",
    "            X = document[ix]\n",
    "            \n",
    "            y = list()\n",
    "            for word in document[floor:ix]:\n",
    "                y.append(word)\n",
    "            for word in document[ix+1:ceil]:\n",
    "                y.append(word)\n",
    "            \n",
    "            yield X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.53013377478298\n",
      "66.3010598636142\n",
      "66.38059388763826\n",
      "67.61330133289258\n",
      "69.79126586773518\n",
      "72.37895503955635\n",
      "74.45427221484351\n",
      "76.02096870847583\n",
      "77.27647062833243\n",
      "78.32555211321612\n"
     ]
    }
   ],
   "source": [
    "class EmbeddingModel(object):\n",
    "    \n",
    "    def __init__(self, vocab_size, h_size, learning_rate, seed=None):\n",
    "        \"\"\"\n",
    "        parameters\n",
    "        ----------\n",
    "        \n",
    "        vocabulary: list of unique words\n",
    "        h_size    : size of the hidden units\n",
    "        \"\"\"\n",
    "        if seed:\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        self.eta = learning_rate\n",
    "        \n",
    "        self.w1   = np.random.randn(vocab_size, h_size)\n",
    "        self.tree = Tree(range(vocab_size), h_size)\n",
    "        \n",
    "        self._dw2 = np.zeros_like(self.tree.weights)\n",
    "    \n",
    "    def feed_forward(self, X, labels):\n",
    "        h = self.w1[None, X]\n",
    "        \n",
    "        u_s = [np.dot(h, self.tree[label].T) for label in labels]\n",
    "        y = [self.sigmoid(u) for u in u_s]\n",
    "\n",
    "        return h, y\n",
    "    \n",
    "    def back_propagation(self, X, labels, y_preds, h):\n",
    "        dw1 = np.zeros((1, self.w1.shape[1]))\n",
    "        self._dw2 *= 0\n",
    "\n",
    "        for label, y_pred in zip(labels, y_preds):\n",
    "            error = [1 if sign == 1 else 0 for sign in self.tree.get_path(label)]\n",
    "            error = y_pred - error\n",
    "            \n",
    "            dw1 += np.dot(error, self.tree[label])\n",
    "            \n",
    "            dw2 = error * h.T\n",
    "            dw2 = dw2.T\n",
    "\n",
    "            for ix, w2_ix in enumerate(self.tree.get_indexes(label)):\n",
    "                self._dw2[w2_ix] += dw2[ix]\n",
    "\n",
    "        self.w1[None, X] -= self.eta * dw1\n",
    "\n",
    "        self.tree.weights -= self.eta * self._dw2\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1./(1.+np.exp(-x))\n",
    "    \n",
    "model = EmbeddingModel(vocab_size, 5, 1e-3, 42)\n",
    "\n",
    "epochs = 15000\n",
    "for epoch in range(epochs):\n",
    "    loss = 0\n",
    "    for X, labels in get_data(corpus_preprocessed):\n",
    "        h, y_preds = model.feed_forward(X, labels)\n",
    "        \n",
    "        for ix, label in enumerate(labels):\n",
    "            for sign, y_pred in zip(model.tree.get_path(label),  y_preds[ix]):\n",
    "                if sign == 1:\n",
    "                    loss += -np.sum(np.log(y_pred))\n",
    "                else:\n",
    "                    loss += -np.sum(np.log(1-y_pred))\n",
    "        \n",
    "        model.back_propagation(X, labels, y_preds, h)\n",
    "        \n",
    "    if epoch % 1500 == 0:\n",
    "        print(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
